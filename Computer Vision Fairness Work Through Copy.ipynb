{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e773bdb",
   "metadata": {},
   "source": [
    "# Computer Vision Fairness Assessment\n",
    "In this lesson we'll go over what is fairness/bias in machine learning, where do these issues come from, and how can we assess a model for fairness and bias. The first two points are covered in this presentation. An approach to assess fairness is presented in this notebook, where we'll loosely follow what [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac5bcd",
   "metadata": {},
   "source": [
    "## Gender Shades Paper Summary\n",
    "In [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf), Buolamwini and Gebru assess disparities in performance of commercial gender classification models across men/women and light/dark skinned individuals. The authors find that performance is significantly worse for women, darker skinned individuals, and especially poor for dark skinned women. This is in contrast to the high performance of these models when measured across the entire dataset. Part of this is due to the imbalanced nature of most computer vision datasets which will have an overrepresentation of men and light skinned individuals.\n",
    "\n",
    "While we are recreating Buolamwini and Gebru's work, there are a few key differences.\n",
    "1. The authors used the Fitzpatrick skin type classification system to determine dark or light skinned individuals because of the variation in skin type within racial/ethnic groups. In this work we're using a coarse Black vs white classification system until I can get access to the [Data Shades dataset](http://gendershades.org/)\n",
    "2. We're using a gender classification system that I trained so that we can also see how to load in and assess [scikit-learn](https://scikit-learn.org/stable/) models, which I assume is more broadly applicable to our work.\n",
    "3. Finally, we're using a different dataset to evaluate on than the original authors had used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68850c09",
   "metadata": {},
   "source": [
    "## Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import hog\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511e533",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c90c4",
   "metadata": {},
   "source": [
    "### How to load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007e9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea8d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42906ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(im):\n",
    "    \"\"\"\n",
    "    Description: Resizes an image to a square and performs some padding.\n",
    "    Input:\n",
    "    Output:\n",
    "    TODO:\n",
    "        1)\n",
    "    \"\"\"\n",
    "    # code snagged from https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
    "    desired_size = 200\n",
    "\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    # im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    # create a new image and paste the resized on it\n",
    "\n",
    "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
    "    new_im.paste(im, ((desired_size-new_size[0])//2,\n",
    "                        (desired_size-new_size[1])//2))\n",
    "    return(new_im)\n",
    "def load_image_data(path_to_data):\n",
    "    \"\"\"\n",
    "    Description: Loads images of men and women from path which contains two folders, man and woman. The data\n",
    "        is loaded and a label is provided depending on where the data came from.\n",
    "    Input:\n",
    "        path_to_data (str): path to folder containing a man and woman folder with images.\n",
    "    Output:\n",
    "        X (4D array): Each row contains an image array (h, w, ch)\n",
    "        Y (1D array): Label for each image.\n",
    "    TODO:\n",
    "        1)\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    inner_folder_list = [\"woman/\", \"man/\"]\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    Y = np.array(Y_list)\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_pool = resize_image(deadpool)\n",
    "small_pool_matrix = np.array(small_pool)\n",
    "plt.imshow(small_pool_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_list = [\"data/train/\", \"data/valid/\", \"data/test/\"]\n",
    "data_dict = {}\n",
    "for path_to_data in path_to_data_list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c70172",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, data_list in data_dict.items():\n",
    "    print(data)\n",
    "    print(\"\\t{},{}\".format(data_list[0].shape, data_list[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc43fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"train\"][0][0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076af9e",
   "metadata": {},
   "source": [
    "## Transform the data\n",
    "Unless we were to work with a model that can take in tensors (saaaayyyyy a neural network) we have to transform our three dimensional images (height, width, channel) into 1-dimensional feature vectors. These features could be as simple as descriptors for the average pixel intensity, the maximum pixel intensity, edge locations, and different combinations of segmentation. We could also flatten an image into a very, very long array. This isn't ideal though and can run into issues, namely the curse of dimensionality.\n",
    "\n",
    "For this example I'm going to use a histogram of oriented gradients, akak HOG. I'm also going to do a lot of hand-waving here because we could use any features really and experimentation is important, but not the focus of this lesson. In short, HOG transforms an image into a one-dimensional feature vector of an image. HOG is similar to edge features in that it can identify where edges are, but it also captures the gradient and orientation. This is done in a localized manner in that the image is segmented. Finally, within each of these segments a histogram of gradient and orientation pixel values is calculated, which become the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c48ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_transform(data_matrix):\n",
    "    \"\"\"\"\"\"\n",
    "    hog_feats, hog_img = hog(\n",
    "        data_matrix, pixels_per_cell=(14,14), \n",
    "        cells_per_block=(2, 2), \n",
    "        orientations=9, \n",
    "        visualize=True, \n",
    "        block_norm='L2-Hys',\n",
    "        multichannel = True)\n",
    "    return(hog_feats, hog_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snagged from https://kapernikov.com/tutorial-image-classification-with-scikit-learn/\n",
    "deadpool_hog_feats, deadpool_hog_img = hog_transform(small_pool_matrix)\n",
    " \n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,6)\n",
    "# remove ticks and their labels\n",
    "[a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False) \n",
    "    for a in ax]\n",
    " \n",
    "ax[0].imshow(small_pool_matrix, cmap='gray')\n",
    "ax[0].set_title('deadpool')\n",
    "ax[1].imshow(deadpool_hog_img, cmap='gray')\n",
    "ax[1].set_title('deadpool hog features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deadpool_hog_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_matrix = data_dict[\"train\"][0][0,:,:,:]\n",
    "# Snagged from https://kapernikov.com/tutorial-image-classification-with-scikit-learn/\n",
    "sample_hog_feats, sample_hog_img = hog_transform(sample_img_matrix)\n",
    " \n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,6)\n",
    "# remove ticks and their labels\n",
    "[a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False) \n",
    "    for a in ax]\n",
    " \n",
    "ax[0].imshow(sample_img_matrix, cmap='gray')\n",
    "ax[0].set_title('Sample Image')\n",
    "ax[1].imshow(sample_hog_img, cmap='gray')\n",
    "ax[1].set_title('sample hog features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}# this will be similar to the data dictionary, but transformed features.\n",
    "for data_name, raw_data_list in data_dict.items():\n",
    "    feature_list = []\n",
    "    X = raw_data_list[0]\n",
    "    for img_idx in tqdm(range(X.shape[0]), desc = data_name):\n",
    "        x_curr = X[img_idx, :, :]\n",
    "        x_feats, _ = hog_transform(x_curr)\n",
    "        feature_list.append(x_feats)\n",
    "    feature_dict[data_name] = feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeae717",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabc490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=23, max_iter=1000, tol=1e-3)\n",
    "sgd_clf.fit(feature_dict[\"train\"], data_dict[\"train\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ccf6b7",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b619f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sgd_clf.predict(feature_dict[\"train\"])\n",
    "precision_recall_fscore_support(y_true=data_dict[\"train\"][1], y_pred=preds, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_performance(clf, X, Y, pos_label = 1):\n",
    "    preds = sgd_clf.predict(X)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true=Y, y_pred=preds, average=\"binary\", pos_label=pos_label)\n",
    "    return(prec, rec, f1, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ddcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, f1, _ = assess_performance(clf = sgd_clf, X = feature_dict[\"train\"], Y = data_dict[\"train\"][1])\n",
    "print(\"precision:{}\\nrecall:{}\\nf1:{}\".format(prec, rec, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, f1, _ = assess_performance(clf = sgd_clf, X = feature_dict[\"valid\"], Y = data_dict[\"valid\"][1])\n",
    "print(\"precision:{}\\nrecall:{}\\nf1:{}\".format(prec, rec, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdedc815",
   "metadata": {},
   "source": [
    "# Assessing Fairness\n",
    "Hey, that's not bad. This model performs this task very well on the training data, and dips a bit on the validation data, but that's to be expected. I bet we could tinker and get the performance boosted as well, but given that we're using a single transformation and the standard model hyper-parameters I'd say we've earned ourselves a nice glass of soda, pop, or sodapop. Whatever your preference.\n",
    "\n",
    "But before we pat ourselves on the back let's go through and look for some performance discrepancies to measure fairness.\n",
    "\n",
    "This data comes from the [UTK: large scale face dataset](https://susanqq.github.io/UTKFace/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_face_data_paths = glob.glob(\"UTK_faces_data/*.jpg\")\n",
    "data_paths_to_use = []\n",
    "data_path_labels = {}\n",
    "for data_path in all_new_face_data_paths:\n",
    "    if len(np.array(data_path.split(\"/\")[-1].split(\"_\"))) == 4:# some of these are no formed well\n",
    "        age, gender, race = np.array(data_path.split(\"/\")[-1].split(\"_\"))[[0,1,2]]\n",
    "        age = int(age)\n",
    "        gender = int(gender)# 0=male, 1=female\n",
    "        race = int(race) # 0=white, 1=Black\n",
    "        if (age <= 80 )and (age >= 20 )and (race in [0, 1]):\n",
    "            data_paths_to_use.append(data_path)\n",
    "            if gender == 0:# flip the gender label. UTK has men as 0, while we use 1\n",
    "                gender = 1\n",
    "            else:\n",
    "                gender = 0\n",
    "            data_path_labels[data_path] = {\"age\":age, \"gender\":gender, \"race\":race}\n",
    "\n",
    "gender_occurrences = []\n",
    "race_occurrences = []\n",
    "for label in [\"gender\", \"race\"]:\n",
    "    for example, label_dict in data_path_labels.items():\n",
    "        gender_occurrences.append(label_dict[\"gender\"])\n",
    "        race_occurrences.append(label_dict[\"race\"])\n",
    "for label, label_occurrences in zip([\"gender\", \"race\"], [gender_occurrences, race_occurrences]):        \n",
    "    print(\"{} counts\".format(label))\n",
    "    print(\"\\t{}\".format(Counter(label_occurrences)))\n",
    "\n",
    "\n",
    "\n",
    "all_features = []\n",
    "gender_labels = []\n",
    "race_labels = []\n",
    "for image_path in tqdm(data_paths_to_use):\n",
    "    image = Image.open(image_path)\n",
    "    image = resize_image(im=image)\n",
    "    data = np.asarray(image)\n",
    "    features, _ = hog_transform(data)\n",
    "    all_features.append(features)\n",
    "    label_dict = data_path_labels[image_path]\n",
    "    gender_labels.append(label_dict[\"gender\"])\n",
    "    race_labels.append(label_dict[\"race\"])\n",
    "X_utk = np.array(all_features)\n",
    "Y_utk = np.array(gender_labels)\n",
    "race_utk = np.array(race_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counter = 0\n",
    "for idx, image_path in tqdm(enumerate(data_paths_to_use)):\n",
    "    label_dict = data_path_labels[image_path]\n",
    "    if label_dict['race'] == 1 and label_dict['gender'] == 0:\n",
    "        if my_counter == 69:\n",
    "            break\n",
    "        my_counter += 1\n",
    "image = Image.open(image_path)\n",
    "image = resize_image(im=image)\n",
    "data = np.asarray(image)\n",
    "sample_hog_feats, sample_hog_img = hog_transform(data)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,6)\n",
    "# remove ticks and their labels\n",
    "[a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False) \n",
    "    for a in ax]\n",
    " \n",
    "ax[0].imshow(data, cmap='gray')\n",
    "ax[0].set_title('Sample Image')\n",
    "ax[1].imshow(sample_hog_img, cmap='gray')\n",
    "ax[1].set_title('sample hog features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142494a1",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d547b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654ff6de",
   "metadata": {},
   "source": [
    "### Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3b365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc24ac85",
   "metadata": {},
   "source": [
    "### Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24072485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a81043a",
   "metadata": {},
   "source": [
    "### Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b86b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e6b4f1",
   "metadata": {},
   "source": [
    "### White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921eada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff69a5e7",
   "metadata": {},
   "source": [
    "### Black Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c72dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = np.array(data_paths_to_use)[mask_Black_women][23]\n",
    "image = Image.open(my_path)\n",
    "image = resize_image(im=image)\n",
    "data = np.asarray(image)\n",
    "sample_hog_feats, sample_hog_img = hog_transform(data)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,6)\n",
    "# remove ticks and their labels\n",
    "[a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False) \n",
    "    for a in ax]\n",
    " \n",
    "ax[0].imshow(data, cmap='gray')\n",
    "ax[0].set_title('Sample Image')\n",
    "ax[1].imshow(sample_hog_img, cmap='gray')\n",
    "ax[1].set_title('sample hog features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104ccbc",
   "metadata": {},
   "source": [
    "### White Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a0b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c902b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = np.array(data_paths_to_use)[mask_white_women][78]\n",
    "image = Image.open(my_path)\n",
    "image = resize_image(im=image)\n",
    "data = np.asarray(image)\n",
    "sample_hog_feats, sample_hog_img = hog_transform(data)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,6)\n",
    "# remove ticks and their labels\n",
    "[a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False) \n",
    "    for a in ax]\n",
    " \n",
    "ax[0].imshow(data, cmap='gray')\n",
    "ax[0].set_title('Sample Image')\n",
    "ax[1].imshow(sample_hog_img, cmap='gray')\n",
    "ax[1].set_title('sample hog features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820039d1",
   "metadata": {},
   "source": [
    "### Black Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655a8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d651f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = np.array(data_paths_to_use)[mask_Black_men][78]\n",
    "image = Image.open(my_path)\n",
    "image = resize_image(im=image)\n",
    "data = np.asarray(image)\n",
    "sample_hog_feats, sample_hog_img = hog_transform(data)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,6)\n",
    "# remove ticks and their labels\n",
    "[a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False) \n",
    "    for a in ax]\n",
    " \n",
    "ax[0].imshow(data, cmap='gray')\n",
    "ax[0].set_title('Sample Image')\n",
    "ax[1].imshow(sample_hog_img, cmap='gray')\n",
    "ax[1].set_title('sample hog features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440d7f7",
   "metadata": {},
   "source": [
    "### White Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198909ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37182e28",
   "metadata": {},
   "source": [
    "## Moving Forward\n",
    "These results were only half of what we were expecting. I had expected the trends found in the paper by Buolamwini and Gebru to continue here. We showed those trends again with Black men and White men, but the results were flipped for Black women and white women. Finding bias in model prediction like this is only the first step. The second is to confirm that you're looking at what you think you're looking at, and then find sources of this bias that you can remedy.\n",
    "\n",
    "1. There's always a chance I did something wrong. Accidentally flipped the gender label or something\n",
    "2. Is our data what we think it is? I plucked some of this data from kaggle and other websites. Are their calssificatio correct? How do they define their categories? Remember, Buolamwini and Gebru used skin type rather than race which can have a lot of variation in skin type.\n",
    "3. We didn't really explore the data like we should have. It would be good to go into our training data and get a sense of the representation of Black and white people. Are there any significant differences in the pictures? Lighting? Location? Centering? Smiling or not?\n",
    "4. The number of Black people in this dataset is quite small and the results are likely noisy. We should collect more data to corect for this, and also run some significance tests.\n",
    "5. Are all models equaly biased? Could tuning our model improve performance at the expense of performance on certain groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8e3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
